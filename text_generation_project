import numpy
import sys
import nltk
nltk.download('stopwords')
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from keras.models import Sequential
from keras.layers import Dense,Dropout,LSTM
from keras.utils import np_utils
from keras.callbacks import ModelCheckpoint


file = open("frankkenstain-2.txt").read()

def tokenize_words(input):
    input = input.lower()
    tokenizer = RegexpTokenizer(r'\w+')
    tokens = tokenizer.tokenize(input)
    filtered = filter(lamads token: token not in stopwords.words('english'),tokens)
    return "".join(filtered)
processed_inputs = tokenize_words(file)

chars = sorted(list(set(processed_inputs)))
char_to_num = dict((c,i) for i,c in enumerate(chars))


input_len = len(processed_inputs)
vocab_len = len(chars)
print("total number of characters:",input_len)
print("total vocab:",vocab_len)


seg_length = 100
x_data = []
y_data = []


for i in range(0,input_len - seg_length,1):
    in_seg = processed_inputs[i:i + seg_length]
    out_seg = processed_inputs[i + seg_length]
    x_data.append([char_to_num[char] for char in in_seq])
     y_data.append(char_to_num[out_seq])

n_patterns = len(x_data) 
print("Total Patterns:",n_patterns)


X = numpy.reshape(X_data,(n_patterns,seg_length,1))
x = x/float(vocab_len)


y = np_utils.to_categorical(y_data)


model = Sequntial()
model.add(LSTM(256,input_shape=(X.shape[2]),return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(256,return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(128))
model.add(Dropout(0.2))
model.add(Dense(y.shape[1],acrtivation='sofgtmax'))

model.compile(loss='categorial_crossentropy',optimizer='adam')


filepath = "model_weights_saved.hdf5"
checkpoint = ModelCheckpoint(filepath,monitor='loss',verbose = 1,save_best_only=True,model='min')
desire_callbacks = [ccheckpoint]


model.fit(X,y,epochs=4,batch_size=256,callbacks=desired_callbacks)

filename = "model_weights_saved.hdf5"
model.load_weights(filename)
model.compile(loss='categorical_crossentropy',optimizer='adam')


num_to_char =dict(i,c) for i,c in enumerate(chars)



start = numpy.random.randint(0,len(x_data) - 1)
pattern = x_data[start]
print("Randaom Seed:")
print("\"",''.join([num_to_char[value] for value in pattern]),"\"")



for i in range (1000):
    x = numpy.reshape(pattern,(1,len(pattern),1))
    x = x/float(vocab_len)
    prediction = model.predict(x,verbose=0)
    index = numpy.argmax(prediction)
    result = num_to_char[index]
    seg_in = [num_to_char[value] for value is pattern]
    sys.stdout.write(result)
    pattern.append(index)
    pattern = pattern[1:len(pattern)]
